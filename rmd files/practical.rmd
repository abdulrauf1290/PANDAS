---
title: "Practicals Based Programming with R (DSM1072)"
author: "ABDUL RAUF"
date: "12/02/2023"
output:
  html_document: default
  pdf_document: 
    toc: yes
    number_sections: yes
  word_document: default
---

__NAME__:-*ABDUL RAUF* \
__ENRL NO__:-*GL6092* \
__ROLL NO__:-*22DSMSA116* \   


# Distributions in R
Probability distribution make the foundation of theory of statistics and data science. Once shape of frequency distribution is decided, every other aspect can be understood very easily.

## Normal distribution---`dnorm(x,mean,sd)`
Suppose we have a guess about the heights of AMU students, say mean height is 168 cm. and its standard deviation is 5 cm. and frequency distribution is bell shaped normal distribution. With these assumptions, we can simulate 500 random heights from normal distribution with `mean=168` and `sd=5` using the R commands:
```{r rnorm_height}
set.seed(1)
height<-rnorm(n=500,mean=168,sd=5)
# Display just 50 random heights
height[1:50]
#compute its mean
mean(height)
#compute its SD
sd(height)
# compute some other features of height
min(height)
max(height)
range(height)
median(height)
#compute percentiles or quantiles
quantile(height,prob=c(0.025,.25,.50,.75,.975))
```
These were the numeric features of the distribution. Now let us study some graphic features:

# Simulation and Fitting of Distributions

## Random number generation

In this chapter we shall study about the __R__ implementations for random number generation and sampling procedures. Fitting of polynomials and exponential curves. Application problems based on fitting of suitable distributions. Normal probability plot.

## Rndom number generation from uniform $U(a,b)$
Random numbers can be generated by the function `runif()`. A general syntax is:
```
runif(n,min=a,max=b) 
```
Execution of this command produces $n$ pseudo random uniform numbers on the interval $[a, b]$. The default values are $a=0$, and $b=1$. The seed is selected internally. However, one can set the seed by using `set.seed()` function.

##Example unifrom deviates
Generate $5$ uniform pseudo random numbers on the interval $[0, 1]$, and $10$ uniform such numbers on the interval $[-3,-1]$.
```{r unif1, message=FALSE,warning=FALSE}
runif(5) # same as runif(5,0,1)
runif(10,min=-3,max=-1)
```
One can use `set.seed()` to generate the same number every time.
```{r runifsetseed1}
set.seed(1)
runif(5)
set.seed(1)
runif(10,-3,-1)
```

## Exercise: random deviates from $U(a,b)$
Use `runif()` with `set.seed(32078)` to generate 10 pseudo random numbers from:

 (a) the uniform $U(0,1)$ distribution
 
```{r}
set.seed(32078)
runif(10)

```
 
 (b) the uniform $U(3,7)$ distribution
 
```{r}
set.seed(32078)
runif(10,3,7)

```
 
 (c) the uniform $U(-2,2)$ distribution.
 
```{r}
set.seed(32078)
runif(10,-2,2)
```
 

## Example on application of simulation
Generate 1000 uniform random variate using `runif()` function, assigning them to a vector called `U`. Do the following:

 (a) Compute the average, variance,and standard deviation of the numbers in `U`.
 
```{r}
set.seed(3)
U=runif(1000)
mean(U) # it should be 0.5
var(U) # it should be 1/12
sd(U) # it should be sqrt(1/12)


```
 
 (b) Compare your results with true mean,variance,and standard deviation.
```{r}
mean=0+1/2 #a+b/2
mean

var=1/12 # (b-a)^2/12
var

sd=sqrt(var)
sd

```
 
 (c) Compute the proportion of values of `U` that are less than 0.6,and compare with the probability that uniform random variable `U` is less than 0.6.
 
```{r}
# note that U<0.6 returns a logical vector,thus
mean(U<0.6) # same as Prob(U<0.6)=punif(0.6),F(x)=x for uniform
```
 
 (d) Estimate the expected value of $\frac{1}{U+1}$.
 
```{r}
mean(1/(U+1))

#histogram of U and x=1/(U+1)
opar<-par(mfrow=c(1,2))
hist(U,prob=TRUE)
hist(1/(U+1),prob=TRUE)

```
 

## Solution using R commands
```{r solution1}
set.seed(19908)
U<-runif(1000)
# Part (a)
mean(U) # it should be 0.5
var(U) # it should be 1/12.
sd(U)  # it should be sqrt(1/12)
# part b
#mean=(a+b)/2
mean=(0+1)/2
mean
#var=((a-b)^2)/12
var=((0+1)^2)/12
var
sd=sqrt(var)
# Part (c)
# Note that U<0.6 returns a logical vector, Thus
mean(U<0.6) # same as Prob(U<0.6)=punif(0.6), F(x)=x for uniform.
# Part (d) E(1/(U+1)) is
mean(1/(U+1))
# histogram of U and x=1/(U+1)
opar<-par(mfrow=c(1,2))
hist(U,prob=TRUE)
hist(1/(U+1),prob=TRUE)
par(opar)
```

Note that the distribution of $U$ is uniform whereas distribution of $\frac{1}{U+1}$ is more close to exponential distribution. These facts will be discussed in more detail in the later part of the course.

## Exercise on uniform $U(3.7,5.8)$
Simulate 10000 independent observations on a uniformly distributed random variable on the interval $[3.7,5.8]$.

```{r}
set.seed(8)
u=runif(10000,3.7,5.8)

```


 (a) Estimate the mean, variance, and standard deviation of such a uniform random variable and compare your estimates with the true values.
 
```{r}

mean(u) # it should be 0.5
var(u) # it should be 1/12
sd(u) # it should be sqrt(1/12)
```

Comparing results with true mean,variance,and standard deviation.
```{r}
mean=(3.7+5.8)/2 #a+b/2
mean

var=(5.8-3.7)^2/12 # (b-a)^2/12
var

sd=sqrt(var)
sd

```


 
 (b) Estimate the probability that such a random variable is greater than 4. Compare it with true value.
 
```{r}
#note that u>4 returns a logical vector,thus
mean(u>4) 
#exact value
1-punif(4)
```
 

## The `sample()` function
The `sample()` function allows you to take a simple random sample from a vector of values. For example, 
```
sample(c(3,5,7),size=2,replace=FALSE)
```
will yield a vector of two values taken (without replacement) from the set `{3,5,3}`. Use the function `sample()` to generate 50 pseudo random integers from 1 through 100:

 (a) sample without replacement

 (b) sample with replacement.
 

## Solution
```{r sample1}
# Part (a)
x<-sample(1:100,size=50) #without replacement
x
# part (b)
y<-sample(1:100,size = 50,replace = TRUE) #with replacement
y
# Note that in y repetitions are allowed, whereas it is not in x.

## The function sample is also used for randomization
sample(c("A","B","C")) #returns randomization.
```

## Distributions in R
Most of the distributions are implemented in `base R`. Remaining are implemented in other packages.

## Normal distribution $N(\mu,\sigma)$
There are four main aspects of a distribution which are implemented in R. They are:

  * `dnorm(x,mean,sd)`, this is the value of the distribution with `mean` and `sd`. 

  * `pnorm(q,mean,sd)`, the value of the CDF at $q$. You know that CDF is defined as $P(X<q)=F(q)$.

  * `qnorm(p,mean,sd)`, this is the value of quantile at probability $p$. Note that quantile is the inverse image of CDF.
  * `rnorm(n,mean,sd)` stands for random simulations from $N(\mu,\sigma)$.

We are illustrating them using **R** commands:
```{r dens1,message=FALSE,warning=FALSE,fig.cap="Standard Normal Distribution"}
# Suppose we want to plot N(0,1).
curve(dnorm(x,mean=0,sd=1),from=-3,to=3)
# Value of density at 0
dnorm(0) # same as 1/sqrt(2*pi)
1/sqrt(2*pi)
abline(v=0,lwd=2)
#value of cdf at 1.96
pnorm(1.96)
abline(v=1.96)
pnorm(-1.96)
abline(v=-1.96)
# Note that area between -1.96 and 1.96 is 0.95
pnorm(1.96)-pnorm(-1.96)
# What should be pnorm(0), it should be 0.5
pnorm(0)
## An example of qnorm(.5)
qnorm(0.5)
qnorm(0.975)
qnorm(0.025)
## An example of 10 random simulation from N(0,1)
rnorm(n=10,mean=0,sd=1)
```

## Exponential distribution
Like normal we can also define exponential distribution:
```
dexp(x,rate) #value of density
pexp(q,rate) #value of cdf
qexp(p,rate) #value of quantile
rexp(n,rate) # random simulation from exponential
```
```{r expr1,message=FALSE,warning=FALSE,fig.cap="Standard Exponential distribution"}
re<-rexp(n=10000,rate=1) #standard exponential
mean(re)
sd(re)
#compute Pr(re>3)
mean(re>3) # probabilty re>3, through simulation
# Check it using pexp
pexp(3,lower.tail = FALSE) #exact
hist(re,prob=TRUE)
curve(dexp(x,rate=1),add=TRUE,lwd=2)
```

Since histogram is made from simulated data and curve is exact, but both are matching. This implies that our simulation is correct.

## Inverse CDF method of simulation
It is a well known result of probability that
$$If\quad X\sim F,cdf\quad of \quad X$$
then
$$F(X)\sim U(0,1)$$
This implies that one can simulate quantiles from $F$ by using the following algorithm:

  (i) Equal the cdf $F(x)=u$ and solve it for $x$, where $u$ is $U(0,1)$ deviate.
 (ii) The equation $x=F^{-1}(u)$ will be a quantile from $F$.

## Simulation from exponential distribution
We can make use of _Inverse CDF_ method for simulation from exponential distribution with $rate=\lambda$. Note that
\begin{equation}
f(x)=\lambda e^{-\lambda x}\quad x>0
\Rightarrow F(x)=1- e^{-\lambda x}
\end{equation}
Thus,
\begin{equation}
 F(x)=u
\Rightarrow 1-e^{-\lambda x}=u
\Rightarrow 1-u=e^{-\lambda x}
\Rightarrow log(1-u)=-\lambda x
\Rightarrow x=-\frac{1}{\lambda}log(1-u)
\end{equation}
Thus __R__ commands are:
```{r rexp1,message=FALSE,warning=FALSE,fig.cap="Exponential density with mean 50 or rate=0.02"}
u<-runif(1000)
lambda<-.02 # value of rate
x<--(1/lambda)*log(1-u) # simulated quantiles
#Check for mean, 1/0.02=50
mean(x) #1/lambda
var(x) #1/lambda^2
sd(x) #1/lambda
mean(x>50) #Prob(X>50)
hist(x,prob=TRUE)
curve(dexp(x,rate=.02),add=TRUE,lwd=2)
abline(v=50,col="red",lwd=2) # vertical line at mean
```
Note that simulated results obtained by using _Inverse CDF_ method are matching with the exact results, both in terms of numeric as well as graphics. 

## Exercise on Simulation from Possion

Estimate the mean and variance of a Poisson random variable whose mean is $7.2$ by simulating $10000$ Poisson pseudo random numbers. Compare your results with theoretical results. (Hint `rpois(n,lambda)`).

```{r,warning=FALSE}
pois=rpois(10000,lambda = 7.2)
mean(pois)
sd(pois)
var(pois)
hist(pois,prob=TRUE)
curve(dpois(x, lambda = 7.2),add = TRUE)

```


## Exercise on simulation from $N(0,1)$

Simulate $10000$ realizations of standard normal random variable $Z$, and use your simulated sample to estimate:

 (a) $P(Z>2.5)$
 
 (b) $P(0<Z<1.645)$
 
 (c) $P(1.2<Z<1.45)$
 
 (d) $P(-1.2<Z<1.3)$
 
Compare your results with exact ones using `pnorm()`.
Hint: 

Z<-rnorm(10000)
(a) mean(Z>2.5)
(b) mean(Z>0 & Z<1.645)

```{r}
z <- rnorm(10000,mean=0,sd=1) #1000 random numbers from a standard normal distribution

#Probabilities using simulated sample


mean( z > 1.5)       #P(Z > 1.5)

mean( z>0 & z<1.5)   #P(0 < Z < 1.5)

mean(z>1.5 & z<1.645) #P(1.5 < Z < 1.645)

mean(z>-0.5 & z< 0.5) #P(−0.5 < Z < 0.5)

#EXACT PROBABLITIES
pnorm(1.5,lower.tail = F)   #P(Z > 1.5)

pnorm(1.5)-pnorm(0)  #P(0 < Z < 1.5)

pnorm(1.645) - pnorm(1.5)  #P(1.5 < Z < 1.645)

pnorm(0.5) - pnorm(-0.5)
```



## Exercise on Simulation from $\chi^2$ distribution
Simulate $1000$ realizations of a $\chi_{(8)}^2$, and estimate its mean and variance. Compare it with theoretical mean (8) and variance (16). Note that for $\chi_{(k)}^2$ the mean is $k$ and
variance is $2k$. Make a histogram of simulated data and overlay exact density curve on it.

Hint: 
```{r}
z<-rchisq(n=1000,df=8)
mean(z)
var(z)
hist(z,prob=TRUE)
curve(dchisq(x,df=8),add = TRUE)
```
## Exercise on simulation from binomial
Simulate 1000 observations from $binomial(10,0.5)$. Compute its mean and variance,and compare it with true mean($10\times 0.5$) and variance $(10\times 0.5\times 0.5)$.

  (a) Make a histogram of simulated observations, and add a Normal curve $N(5,2.5)$ on it.
  (b) Comment on the closeness of histogram and Normal curve. Do you feel that binomial probability can be approximated by Normal probability? 

```{r warning=FALSE}
bin<-rbinom(n=1000,size=10,prob=0.5)
mean(bin) # mean(10*0.5=0.5)
var(bin) #variance(10*0.5*0.5=1.0)

10*0.5 #true mean
10*0.5*0.5 #true variance

hist(bin,prob=TRUE)
curve(dbinom(x,size=10,prob=0.5),add=TRUE,col="brown")
```

## Law of large numbers
It states that as 
\begin{equation}
 n\rightarrow \infty \Rightarrow \bar{x}\stackrel{p}\rightarrow \mu
\end{equation}
Thus for large sample size, sample mean ($\bar{x}$) converges to population mean ($\mu$).

## Height of American women
In this example we simulate heights of American women from $N(64.5, 2.5)$ try to see the behavior of sample mean which converges to population mean as sample size becomes large. This phenomenon has been illustrated in the following __R__ commands.
```{r lawlarge1,messgae=FALSE,warning=FALSE}
# generate sample means for different sample sizes.
xbar<-sapply(1:10000, function(n) mean(rnorm(n,64.5,2.5)))
plot(xbar,xlim=c(0,10000))
abline(h=64.5,lwd=2,col="blue")
```

Note that we have used a new function `sapply()` whose first argument is a vector or listed data and second argument is the expression which calls the first argument repeatedly. For example, in the above situation `sapply()` repeatedly calculates mean of random sample of sizes $1:10000$. 

## Sampling distribution---`replicate()`
Note that _statistic_ is a function of sampled values, and distribution of a statistic is called _sampling distribution_. In __R__, the function `replicate()` is an implementation of sampling distribution. Its syntax is:
```
replicate(n,expr)
```
## Sampling from normal dsitribution
If $x_i\sim N(0,1)$,  for $i=1\cdots n,$then $\bar{x}\sim N(0,\frac{1}{n}$. Note that $n$ is not an argument of `replicate()`. We can express this fact using the function `replicate()` for $n=10$, in the following __R__ command. This command will simulate $n=10$ random observations from $N(0,1)$ and then computes its mean. This process is repeated $1000$ times, resulting $1000$ sample means from $N(0,1)$. We can verify the generated sample numerically as well as graphically.

```{r replicate_norm01, message=FALSE, warning=FALSE}
xbar<-replicate(1000,mean(rnorm(10)))
mean(xbar) # zero
sd(xbar) #1/sqrt(10)
```

## Graphical illustration of sampling from $N(0,1)$
```{r graphic_samplingN01,message=FALSE,warning=FALSE,fig.cap="Sampling distribution from N(0,1) for a sample of size 10."}
xbar<-replicate(1000,mean(rnorm(10)))
hist(xbar,prob=TRUE)
curve(dnorm(x,mean = 0,sd=1/sqrt(10)),add=TRUE,lwd=1)
```

## Central limit theorem (CLT)
If $x_i\sim (\mu,\sigma^2)$ for $i=1,\cdots,n$ $\Rightarrow  \bar{x}\stackrel{L}\rightarrow N(\mu,\frac{\sigma^2}{n})$. This is known as _central limit theorem_. Note that in __CLT__ even if parent population is not known the sampling distribution of mean can be approximated by a normal distribution $N(\mu,\sigma^2/n)$. 

## CLT when parent is _standard exponential_
Suppose that parent population is standard exponential, and a sample of size $10$ is taken to see the sampling behavior of the sample mean.This means that:
\begin{equation}
f(x)=\lambda e^{-\lambda x},\;
E(x)=\frac{1}{\lambda},\;
Var(x)=\frac{1}{\lambda^2},\;
SD(x)=\frac{1}{\lambda}
\end{equation}
Thus for standard exponential 
\begin{equation}
f(x)=e^{-x},\; mean=1,\;sd=1,\;
E(\bar{x})=1,\; SD(\bar{x})=\frac{1}{\sqrt{n}}
\end{equation}
Using CLT one can approximate the sampling distribution of sample mean by a normal distribution $N(mean=1,sd=1/\sqrt{10})$.  Note that exact distribution of sample mean is gamma distribution with $shape=10$ and $rate=10$. Keeping in view these facts we sue the following set of __R__ commands:
```{r cltexp10,message=FALSE,warning=FALSE,fig.cap="Comparison of exact and approximate sampling distributions of sampled mean (n=10) from standard exponential."}
#simulate 1000 sample means of size 10 from dexp(x,rate=1)
xbare10<-replicate(1000,expr=mean(rexp(10)))
mean(xbare10) #mean=1
sd(xbare10) #1/sqrt(10)
hist(xbare10,prob=TRUE,ylim=c(0,1.4))
curve(dnorm(x,1,1/sqrt(10)),add=TRUE)
curve(dgamma(x,shape=10,rate=10),add=TRUE,lty=2)
legend("topright",legend = c("Normal approximation","Exact"),lty=c(1,2))
```
Note that approximation is not excellent as $n=10$ only. See what happens when $n=100$. We expect that it will improve the approximation. Whole story can bee seen in the following set of commands:
```{r cltexp100,message=FALSE,warning=FALSE,fig.cap="Comparison of exact and approximate sampling distributions of sampled mean (n=100) from standard exponential."}
#simulate 1000 sample means of size 100 from dexp(x,rate=1)
xbare100<-replicate(1000,expr=mean(rexp(100)))
mean(xbare100) #mean=1
sd(xbare100) #1/sqrt(100)
hist(xbare100,prob=TRUE,ylim=c(0,4))
curve(dnorm(x,1,1/sqrt(100)),add=TRUE)
curve(dgamma(x,shape=100,rate=100),add=TRUE,lty=2)
legend("topright",legend = c("Normal approximation","Exact"),lty=c(1,2))
```

## Student's t distribution with $5$ dgrees of freedom
If $$t=\frac{z}{\sqrt{x/k}}\sim t_k$$
then it is termed as Students't distribution with $k$ degrees of freedom. It may be noted that $z\sim N(0,1)$ and $x\sim \chi_k^2$, that is $\chi^2$ with $k$  degrees of freedom. It must be noted that both $z$ and $x$ are independent random variables. We implement this sampling distribution with $k=5$ degrees of freedom as:
```{r t5,message=FALSE,warning=FALSE,fig.cap="Students,t distribution with 5 degrees of freedom"}
t5<-replicate(n=1000,expr=rnorm(1)/sqrt(rchisq(1,df=5)/5))
hist(t5,prob=TRUE,ylim=c(0,0.4),xlim=c(-5,5))
curve(dt(x,df=5),add=TRUE)
```
Note that histogram is constructed on simulated values using `replicate()` whereas density curve is exact $t$ with $5$ degrees of freedom. However, both are matching, which is an indication of the fact that both follow $t$ distribution with $5$ degrees of freedom.

## $\chi_k^{2}$ distribution with $k$ degrees of freedom
It is a well known fact that if $z\sim N(0,1)$ then $z^2\sim \chi_1^{2}$, that is $\chi^2$ distribution with single degree of freedom. Similarly, $\chi_5^{2}$ distribution with $5$ degrees of freedom is defined as if $$z_i\sim N(0,1),\;i=1,\cdots,5$$ then
$$\Sigma_{i=1}^{i=5}z_i^2\sim \chi_5^{2}$$
This fact can be implemented using __R__ commands as:
```{r chisquare5,message=FALSE,warning=FALSE,fig.cap="Chi-square  sampling distribution with 5 degrees of freedom."}
chisq5<-replicate(1000,expr=sum(rnorm(5)^2))
hist(chisq5,prob=TRUE)
curve(dchisq(x,df=5),add=TRUE)
```
It is evident from above figure that simulated and exact curve are very close, depicting the fact that both represent the same distribution.

## Exercise on $F(2,12)$
Define $F$ distribution with $2$ and $12$ degrees of freedoms. Implement that using the `replicate()` function, and support your findings graphically.
```{r}
fdis=replicate(10000,(rchisq(1,df=2)/2))/(rchisq(1,df=12)/12)
mean(fdis) # mean= d2/(d2-d1)
hist(fdis,prob=TRUE,ylim=c(0,4))
curve(df(x,df1=2,df2= 12),add=TRUE,col="red")
```


## Integration using simulation for _beta_ distribution
Compute $E(X)$ where $x\sim beta(shape1=3,shape2=4)$ using simulation and verify that by using `integrate()` function of __R__
```{r rbeta34,message=FALSE,warning=FALSE}
x<-rbeta(10000,shape1=3,shape2=4)
mean(x) #(3/(3+4))
#Verification using integrate
# define a function for integrand of E(x)
f1<-function(x,shape1,shape2) x*dbeta(x,shape1,shape2)
integrate(f1,shape1=3,shape2=4,lower=0,upper=1)
```
Note that there is excellent agreement between the two results obtained from simulation as well as integration.


## Analysis with `ggformula`
```{r gg1}
library(ggformula)
gf_point(mpg ~ hp, data = mtcars)
library(dplyr)
mtcars %>% gf_point(mpg ~ hp)
gf_point(mpg ~ hp, color = ~ cyl, size = ~ carb, alpha = 0.50, data = mtcars) 
gf_point(mpg ~ hp,  color = ~ factor(cyl), size = ~ carb, alpha = 0.75, data = mtcars)
mtcars %>% 
  mutate(cylinders = factor(cyl)) %>% 
  gf_point(mpg ~ hp,  color = ~ cylinders, size = ~ carb, alpha = 0.75)
data(penguins, package = "palmerpenguins")  
gf_density( ~ bill_length_mm, data = penguins)
gf_density( ~ bill_length_mm,  fill = ~ species,  alpha = 0.5, data = penguins)
# gf_dens() is similar, but there is no line at bottom/sides and the plot is not fillable
gf_dens( ~ bill_length_mm, color = ~ species,  alpha = 0.7, data = penguins)
# gf_dens2() is like gf_dens() but is fillable
gf_dens2( ~ bill_length_mm, fill = ~ species, data = penguins,
          color = "gray50", alpha = 0.4) 
# less smoothing
penguins %>% gf_dens( ~ bill_length_mm, color = ~ species, alpha = 0.7, adjust = 0.25)  
# more smoothing
penguins %>% gf_dens( ~ bill_length_mm, color = ~ species, alpha = 0.7, adjust = 4)     

```



# Analysis with ggformula

__ggformula__ is a package which has interface with __ggplot2__. It is a kind of formula interface like __lattice__ package. 

## gf_boxplot 
```{r ghbox1}
require(ggformula)
require(mosaicData)
gf_boxplot(age~substance,data=HELPrct)
gf_boxplot(age~substance,data=HELPrct,color=~sex)
```

## gf_col
```{r gfcol1}
SomeData<-data.frame(group=LETTERS[1:3],count=c(20,25,18))
SomeData
gf_col(count~group,data=SomeData)
```

## gf_curve
```{r gfcurve1}
D=data.frame(x1=2.62,x2=3.57,y1=21.0,y2=15.0)
D
gf_point(mpg~wt,data=mtcars)%>%
  gf_curve(y1+y2~x1+x2,data=D,color="navy")%>%
  gf_segment(y1+y2~x1+x2,data=D,color="red")
```

## gf_density()

```{r gfdensity1}
gf_dens()
require(palmerpenguins)
data(penguins,package="palmerpenguins")
gf_dens(~bill_length_mm,color=~species,data=penguins)
gf_dens2(~bill_length_mm,color=~species,data=penguins)
```

## Plot distributions--`gf_dist`
```{r dist1}
gf_dhistogram(~rnorm(100),bins=20)%>%gf_dist("norm",col="red")
# shading tails
gf_dist("norm",fill=~(abs(x)<=2),geom="area")
gf_dist("norm",color="red",kind="cdf")
gf_dist("norm",color="red",kind="histogram")
## an example from a discrete distribution
gf_dist("binom",size=20,prob=0.25)
gf_dist("binom",params = list(size=20,prob=.25),size=2)
```


## Fiting of distributions---`optim()`
```{r fitexp}
nlle<-function(theta,y){
  ll<-dexp(y,rate=theta,log=TRUE)
  ll<-sum(ll)
  return(-ll)
}
dump("nlle",file="nlle.txt")
y<-rexp(30,rate=0.02)
M1=optim(par=c(.01),fn=nlle,hessian=TRUE,y=y)
MLE=M1$par
se=sqrt(diag(solve(M1$hessian)))
MLE
se
out<-cbind(MLE,se)
rownames(out)<-c("rate")
round(out,3)
# Fittting of Weibull
nllw<-function(theta,y){
  ll=dweibull(y,shape=theta[1],scale=theta[2],log=TRUE)
  ll<-sum(ll)
  return(-ll)
}
y=rweibull(35,shape=1.2,scale=50)
y
M2<-optim(par=c(1,30),fn=nllw,hessian = T,y=y,method="L-BFGS-B",lower=c(.01,2))
M2
MLE=M2$par
se=sqrt(diag(solve(M2$hessian)))
out=cbind(MLE,se)
rownames(out)=c("shape","scale")
out
```
**Fit gamma distribution**

# Analysis of `swiss` data

Let us discuss regression analysis of `swiss` data which is available with **base R** [@R-base]. This data set tells about standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. The format of the data is a data frame with 47 observation on 6 variables,each of which is in percent, i.e, in [0,100]. The description of varibles are:

**Fertility**: I~g~, 'common standardized fertility measure'

**Agriculture**: % of males involved in agriculture as occupation

**Examination**: % draftees receiving highest marks on army examination

**Education**: % education beyond primary school for draftees

**Catholic**: % catholic (as opposed to protestant)

**Infant.Mortality**: live births who live less than 1 year.

## Summary features of `swiss` data 

We shall talk about numeric as well as graphic summaries. 

```{r swiss1}
names(swiss)
head(swiss)
str(swiss)
# summary of the data
summary(swiss)
```

## Now look into graphics summary

```{r swiss_graph1, fig.cap="Scatter plot matrix of `swiss`"}
par(mfrow=c(1,1),mar=c(4,4,1,.1),mgp=c(2,.75,0),bg="white")
pairs(swiss)
```

A better graphic view can be obtained using a function in the panel. Let us define this function
```{r pnael1}

pan1<-function(x,y){
  points(x,y,pch=20)#add points for scatter plot
  m1<-lm(y~x)#fit a simple regression
  abline(m1,lwd=2)#add a fitted line in each panel
}

dump("pan1", file="pan1.txt") #dump it
source("pan1.txt") #source it
```

Let us have the graph with added line^[to see the linear trend in each panel] in the panel^[from the graph it is evident that there is positive trend between `Fertility`, and `Examination`, whereas there is negative trend between `Agriculture` and `Examination`, and also a negative trend between `Fertility` and `Examination`].
```{r swiss_graph2, fig.cap="Scatter plot matrix of `swiss`"}
par(mfrow=c(1,1),mar=c(4,4,1,.1),mgp=c(2,.75,0),bg="white")
pairs(swiss,panel=pan1)
```

## Fit a multiple linear regression model for `swiss` data 

We fit a model using `Fertility` as a response and other variables remaining in the data frames as regressor. We put a dot after the tilde in the formula which means that all variables in the data frame `swiss` except `Fertility` will be treated as regressor.

```{r}
Mfull<-lm(Fertility~., data=swiss)
summary(Mfull)
```

From above output it is evident that `Examination` is not linearly related with `Fertility` as the corresponding pvalue $Pr(>|t|)$ is not less than .05. 

## Analysis with __arm__

We can get a nice summary of results using the function `display()` of __arm__ package[@R-arm]. Moreover, a nice graphic display can also be obtained using the function `coefplot()` of the __arm__ package again. It may be noted that graphic and numeric summaries reach to the same conclusion.

```{r swiss_graph3, fig.cap="Caterpiller plot of swiss data"}
par(mar=c(4,8,4,1),mgp=c(2,1,0),bg="white")
#Let us have the display of summary using display()
library(arm)
display(Mfull, detail=TRUE)
coefplot(Mfull)
```

# Model comparision---`anova()`

Model comparison here means comparison of the nested models. Note that `anova()` is meant for displaying analysis of variance when there is only one fitted model object in its argument. However, when there are more than one models in its argument it provides a model comparison among the nested models. Following illustration makes the things more clear. 

```{r swiss_anova1}

#Fit the model with all predictors
M_full<-lm(Fertility~., data=swiss)
#Fit a model in which `Examination` is dropped 
M_exam<-lm(Fertility~Agriculture+Education+Catholic+Infant.Mortality, data=swiss)

#compare these two nested models
anova(Mfull, M_exam)
```

From above output it is evident that when `Examination` is dropped from full model `Mfull` the `pvalue`, which is .3155 indicates that there is no harm in dropping the `Examination` from the model. This fact can be seen in the graphic display also.
```{r swiss_graph4, fig.cap="Caterpiller plot of swiss data when `Examination` is dropped from the model"}
par(mar=c(4,8,4,1),mgp=c(2,1,0),bg="white")
#Let us have the display of summary using display()
library(arm)
display(M_exam, detail=TRUE)
coefplot(M_exam)
```

## Stepwise regression---`stepAIC()`in __MASS__

A big question is that which terms of the model should be kept in the final model, particularly when our number of predictors are large. The method is stepwise regression, in which we may start from a full model and the terms which are not changing the AIC significantly. This is known as *backward* method. Alternatively, we may start with a smaller model and add the terms accordingly. This is known as *forward* method. However, we may add or drop at a step, this is known as *both* direction. The function `stepAIC()`available with **MASS** [@R-MASS] uses an automated method `both` and final model is reported. Note that AIC is defined as:

$$AIC=-2*max.loglikelihood+2*nparameters$$

```{r step1}
require(MASS)
M_best<-stepAIC(M_full,trace=FALSE)
M_best
```











